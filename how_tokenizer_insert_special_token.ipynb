{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaNyJJghgK+fV6BOzxui+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomoyahiroe/transformers-playground/blob/main/how_tokenizer_insert_special_token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "zCW5bNTQxLcz",
        "outputId": "040337e8-c554-4cd6-a95b-5febdf7d1f82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[ja,sentencepiece,torch] in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (4.67.1)\n",
            "Requirement already satisfied: fugashi>=1.0 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (1.5.1)\n",
            "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (1.0.0)\n",
            "Requirement already satisfied: unidic-lite>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (1.0.8)\n",
            "Requirement already satisfied: unidic>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (1.1.0)\n",
            "Requirement already satisfied: sudachipy>=0.6.6 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (0.6.10)\n",
            "Requirement already satisfied: sudachidict-core>=20220729 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (20250515)\n",
            "Requirement already satisfied: rhoknp<1.3.1,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (5.29.5)\n",
            "Requirement already satisfied: torch<2.7,>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[ja,sentencepiece,torch]) (1.8.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[ja,sentencepiece,torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[ja,sentencepiece,torch]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[ja,sentencepiece,torch]) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[ja,sentencepiece,torch]) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (1.3.0)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from unidic>=1.0.2->transformers[ja,sentencepiece,torch]) (0.10.1)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from unidic>=1.0.2->transformers[ja,sentencepiece,torch]) (1.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7,>=2.1->transformers[ja,sentencepiece,torch]) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# transformersのインストール\n",
        "!pip install transformers[ja,sentencepiece,torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
        "import pprint\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZoKbS8_Hx9dy"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## どのようにTokenizerは特殊トークンを挿入しているのか"
      ],
      "metadata": {
        "id": "0gnQRNOTLy8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cl-tohoku/bert-base-japanese-v3の例"
      ],
      "metadata": {
        "id": "PRaWMHeTR9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v3\")\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "m5kar63Sx_bT",
        "outputId": "fcf56fd4-0b73-4e72-c76f-9f82a6c525ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unk_token': '[UNK]',\n",
              " 'sep_token': '[SEP]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 文頭と文末に特殊トークンが挿入される例"
      ],
      "metadata": {
        "id": "h0A6n7RAMlvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"今日は良い天気ですね。\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "JmPH_d5q-Qid",
        "outputId": "7117be74-0fce-4987-abe0-1cd0b18128b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 複数の文章をひとまとまりの文字列として入力しても、途中に`[SEP]`は挿入されない"
      ],
      "metadata": {
        "id": "90fbQEqQMnmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"今日は良い天気ですね。公園で本でも読みましょうか\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "5FmjeDcEAs7u",
        "outputId": "eaf87b44-ce0c-45ec-fabd-5045fa3c973c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '公園', 'で', '本', 'で', 'も', '読み', 'ましょう', 'か', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 一文毎に分割して配列にして渡せば、`[SEP]`が入力されるが、`[CLS]`も入力される"
      ],
      "metadata": {
        "id": "29H_oUQKM12p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"今日は良い天気ですね。\",\"公園で本でも読みましょうか\"]\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# input_ids をトークンにデコードして確認します\n",
        "for i, input_id_list in enumerate(encoded_inputs[\"input_ids\"]):\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    print(f\"original text: {text[i]}\")\n",
        "    print(f\"model input(decoded): {decoded_tokens}\")\n",
        "    print(f\"attention mask: {encoded_inputs['attention_mask'][i].tolist()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "V4VGjnpVyJsx",
        "outputId": "f1c10a3c-98c4-4e33-9c08-8c9c7ac2d268",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: 今日は良い天気ですね。\n",
            "model input(decoded): ['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '[SEP]', '[PAD]']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
            "--------------------\n",
            "original text: 公園で本でも読みましょうか\n",
            "model input(decoded): ['[CLS]', '公園', 'で', '本', 'で', 'も', '読み', 'ましょう', 'か', '[SEP]']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenizerの引数に二つ入力することで、`[SEP]` のみを挿入できる"
      ],
      "metadata": {
        "id": "PJ9SocgKN5bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"今日は良い天気ですね。\"\n",
        "text2 = \"公園で本でも読みましょうか\""
      ],
      "metadata": {
        "id": "O-MOQ92CsDLD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "iY1FiulPr_m0",
        "outputId": "5f546b7c-0f15-49cd-d597-3fba5aa08617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '[SEP]', '公園', 'で', '本', 'で', 'も', '読み', 'ましょう', 'か', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## abeja/gpt2-large-japaneseの例"
      ],
      "metadata": {
        "id": "I50xoThASGup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt2-large-japanese\")\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "yWKxx09nSJg3",
        "outputId": "7124a5ed-bdec-41b4-826d-a6a7a895f92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bos_token': '<s>',\n",
              " 'eos_token': '</s>',\n",
              " 'unk_token': '<unk>',\n",
              " 'sep_token': '[SEP]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"日本の首都は\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)\n",
        "pprint.pp(encoded_inputs)"
      ],
      "metadata": {
        "id": "s2dL7jEeVoLv",
        "outputId": "de128826-0867-4682-cb62-2d8035024021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁日本', 'の首都', 'は', '</s>']\n",
            "{'input_ids': tensor([[  491, 11308,    15,     2]]),\n",
            " 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"アメリカの首都はワシントンD.C.です。日本の首都は\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "dHR41K54WBRj",
        "outputId": "4d854f33-5e4e-47d3-a18e-3007be3d3657",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '日本の', '首都', 'は', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"アメリカの首都はワシントンD.C.です。\", \"日本の首都は\"]\n",
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "0LzqrlatYKC0",
        "outputId": "76136bfb-da14-491b-da7d-d188f26f87cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '</s>', '▁', '公園', 'で', '本', 'でも', '読み', 'ましょう', 'か', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"アメリカの首都はワシントンD.C.です。\"\n",
        "text2 = \"日本の首都は\"\n",
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "uFcsKO3-XLW6",
        "outputId": "63cd2d82-7032-4aca-dbee-a689dfd573fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '</s>', '▁日本', 'の首都', 'は', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章を複数入力したい２つのシチュエーションについて\n",
        "\n",
        "1. 一つの出力を得たいが、BERTに文章を区別してもらいたい\n",
        "\n",
        "2. 一つ一つの文章毎に、BERTに特定のタスクを行ってもらいたい"
      ],
      "metadata": {
        "id": "YWzgxUQJL5HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt2-large-japanese\")"
      ],
      "metadata": {
        "id": "B2UAXM_zPMB_"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一つの出力を得たい"
      ],
      "metadata": {
        "id": "GC-yZfCWO-SS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenizerの引数に２つの文字列を渡すパターン"
      ],
      "metadata": {
        "id": "rXlEpHr8p-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"アメリカの首都はワシントンD.C.です。\"\n",
        "question = \"そして、日本の首都は\""
      ],
      "metadata": {
        "id": "AxEadSv6NIZ9"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(context, question, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "Dg91v2_bPiCq",
        "outputId": "aa9ad2b7-56e0-41c4-8fac-9b38bf27e26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '</s>', '▁そして', '、', '日本の', '首都', 'は', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "FcY-fCg5Pnkp"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "_9ODfDZ2mCAS",
        "outputId": "1f75cbe1-61ec-48e8-ae3f-33e2e0915d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['アメリカの首都はワシントンD.C.です。 そして、日本の首都は首都機能が']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenizerに配列を渡すパターン"
      ],
      "metadata": {
        "id": "jBFQPBG_qFl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"アメリカの首都はワシントンD.C.です。\"\n",
        "question = \"そして、日本の首都は\"\n",
        "text = [context, question]"
      ],
      "metadata": {
        "id": "7BUdfxTArARI"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "for i, input_id_list in enumerate(encoded_inputs[\"input_ids\"]):\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    print(f\"original text: {text[i]}\")\n",
        "    print(f\"model input(decoded): {decoded_tokens}\")\n",
        "    print(f\"attention mask: {encoded_inputs['attention_mask'][i].tolist()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "hDr0yDT9qOqR",
        "outputId": "cea170ec-4265-415b-a2c1-57e913996bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: アメリカの首都はワシントンD.C.です。\n",
            "model input(decoded): ['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '</s>']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------\n",
            "original text: そして、日本の首都は\n",
            "model input(decoded): ['▁そして', '、', '日本の', '首都', 'は', '</s>', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "6TJGAKv6qSft",
        "outputId": "87b2dffb-041f-4794-a5bc-6c71bf26d7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "g4lB_xwgqqyY",
        "outputId": "dcb9c0ca-23bc-432b-bbe2-ec4633d7a62d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['アメリカの首都はワシントンD.C.です。ワシントンD.C.は、アメリカ合衆国の首都', 'そして、日本の首都は東京 ですが、 首都は ']\n"
          ]
        }
      ]
    }
  ]
}