{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": false,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaNyJJghgK+fV6BOzxui+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomoyahiroe/transformers-playground/blob/main/how_tokenizer_insert_special_token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCW5bNTQxLcz"
      },
      "outputs": [],
      "source": [
        "# transformersのインストール\n",
        "!pip install transformers[ja,sentencepiece,torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
        "import pprint\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZoKbS8_Hx9dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## どのようにTokenizerは特殊トークンを挿入しているのか"
      ],
      "metadata": {
        "id": "0gnQRNOTLy8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cl-tohoku/bert-base-japanese-v3の例"
      ],
      "metadata": {
        "id": "PRaWMHeTR9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v3\")\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "m5kar63Sx_bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 文頭と文末に特殊トークンが挿入される例"
      ],
      "metadata": {
        "id": "h0A6n7RAMlvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"今日は良い天気ですね。\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "JmPH_d5q-Qid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 複数の文章をひとまとまりの文字列として入力しても、途中に`[SEP]`は挿入されない"
      ],
      "metadata": {
        "id": "90fbQEqQMnmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"今日は良い天気ですね。公園で本でも読みましょうか\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "5FmjeDcEAs7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 一文毎に分割して配列にして渡せば、`[SEP]`が入力されるが、`[CLS]`も入力される"
      ],
      "metadata": {
        "id": "29H_oUQKM12p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"今日は良い天気ですね。\",\"公園で本でも読みましょうか\"]\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# input_ids をトークンにデコードして確認します\n",
        "for i, input_id_list in enumerate(encoded_inputs[\"input_ids\"]):\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    print(f\"original text: {text[i]}\")\n",
        "    print(f\"model input(decoded): {decoded_tokens}\")\n",
        "    print(f\"attention mask: {encoded_inputs['attention_mask'][i].tolist()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "V4VGjnpVyJsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenizerの引数に二つ入力することで、`[SEP]` のみを挿入できる"
      ],
      "metadata": {
        "id": "PJ9SocgKN5bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"今日は良い天気ですね。\"\n",
        "text2 = \"公園で本でも読みましょうか\""
      ],
      "metadata": {
        "id": "O-MOQ92CsDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "iY1FiulPr_m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## abeja/gpt2-large-japaneseの例"
      ],
      "metadata": {
        "id": "I50xoThASGup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt2-large-japanese\")\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "yWKxx09nSJg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"日本の首都は\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)\n",
        "pprint.pp(encoded_inputs)"
      ],
      "metadata": {
        "id": "s2dL7jEeVoLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"アメリカの首都はワシントンD.C.です。日本の首都は\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "dHR41K54WBRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"アメリカの首都はワシントンD.C.です。\", \"日本の首都は\"]\n",
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "0LzqrlatYKC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"アメリカの首都はワシントンD.C.です。\"\n",
        "text2 = \"日本の首都は\"\n",
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "uFcsKO3-XLW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章を複数入力したい２つのシチュエーションについて\n",
        "\n",
        "1. 一つの出力を得たいが、BERTに文章を区別してもらいたい\n",
        "\n",
        "2. 一つ一つの文章毎に、BERTに特定のタスクを行ってもらいたい"
      ],
      "metadata": {
        "id": "YWzgxUQJL5HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt2-large-japanese\")"
      ],
      "metadata": {
        "id": "B2UAXM_zPMB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一つの出力を得たい"
      ],
      "metadata": {
        "id": "GC-yZfCWO-SS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenizerの引数に２つの文字列を渡すパターン"
      ],
      "metadata": {
        "id": "rXlEpHr8p-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"アメリカの首都はワシントンD.C.です。\"\n",
        "question = \"そして、日本の首都は\""
      ],
      "metadata": {
        "id": "AxEadSv6NIZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(context, question, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "Dg91v2_bPiCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "FcY-fCg5Pnkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "_9ODfDZ2mCAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenizerに配列を渡すパターン"
      ],
      "metadata": {
        "id": "jBFQPBG_qFl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"アメリカの首都はワシントンD.C.です。\"\n",
        "question = \"そして、日本の首都は\"\n",
        "text = [context, question]"
      ],
      "metadata": {
        "id": "7BUdfxTArARI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "for i, input_id_list in enumerate(encoded_inputs[\"input_ids\"]):\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    print(f\"original text: {text[i]}\")\n",
        "    print(f\"model input(decoded): {decoded_tokens}\")\n",
        "    print(f\"attention mask: {encoded_inputs['attention_mask'][i].tolist()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "hDr0yDT9qOqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "6TJGAKv6qSft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "g4lB_xwgqqyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
