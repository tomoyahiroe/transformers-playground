{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqTZWOMRF77TBe20SNWE3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomoyahiroe/transformers-playground/blob/main/how_tokenizer_insert_special_token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCW5bNTQxLcz"
      },
      "outputs": [],
      "source": [
        "# transformersのインストール\n",
        "!pip install transformers[ja,sentencepiece,torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
        "import pprint\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZoKbS8_Hx9dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## どのようにTokenizerは特殊トークンを挿入しているのか"
      ],
      "metadata": {
        "id": "0gnQRNOTLy8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cl-tohoku/bert-base-japanese-v3の例"
      ],
      "metadata": {
        "id": "PRaWMHeTR9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v3\")\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "m5kar63Sx_bT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf56fd4-0b73-4e72-c76f-9f82a6c525ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unk_token': '[UNK]',\n",
              " 'sep_token': '[SEP]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 文頭と文末に特殊トークンが挿入される例"
      ],
      "metadata": {
        "id": "h0A6n7RAMlvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"今日は良い天気ですね。\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "JmPH_d5q-Qid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7117be74-0fce-4987-abe0-1cd0b18128b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 複数の文章をひとまとまりの文字列として入力しても、途中に`[SEP]`は挿入されない"
      ],
      "metadata": {
        "id": "90fbQEqQMnmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"今日は良い天気ですね。公園で本でも読みましょうか\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "5FmjeDcEAs7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf87b44-ce0c-45ec-fabd-5045fa3c973c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '公園', 'で', '本', 'で', 'も', '読み', 'ましょう', 'か', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 一文毎に分割して配列にして渡せば、`[SEP]`が入力されるが、`[CLS]`も入力される"
      ],
      "metadata": {
        "id": "29H_oUQKM12p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"今日は良い天気ですね。\",\"公園で本でも読みましょうか\"]\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# input_ids をトークンにデコードして確認します\n",
        "for i, input_id_list in enumerate(encoded_inputs[\"input_ids\"]):\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    print(f\"original text: {text[i]}\")\n",
        "    print(f\"model input(decoded): {decoded_tokens}\")\n",
        "    print(f\"attention mask: {encoded_inputs['attention_mask'][i].tolist()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "V4VGjnpVyJsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c10a3c-98c4-4e33-9c08-8c9c7ac2d268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: 今日は良い天気ですね。\n",
            "model input(decoded): ['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '[SEP]', '[PAD]']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
            "--------------------\n",
            "original text: 公園で本でも読みましょうか\n",
            "model input(decoded): ['[CLS]', '公園', 'で', '本', 'で', 'も', '読み', 'ましょう', 'か', '[SEP]']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenizerの引数に二つ入力することで、`[SEP]` のみを挿入できる"
      ],
      "metadata": {
        "id": "PJ9SocgKN5bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"今日は良い天気ですね。\"\n",
        "text2 = \"公園で本でも読みましょうか\""
      ],
      "metadata": {
        "id": "O-MOQ92CsDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "iY1FiulPr_m0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f546b7c-0f15-49cd-d597-3fba5aa08617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '[SEP]', '公園', 'で', '本', 'で', 'も', '読み', 'ましょう', 'か', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### abeja/gpt2-large-japaneseの例"
      ],
      "metadata": {
        "id": "I50xoThASGup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt2-large-japanese\")\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "yWKxx09nSJg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7124a5ed-bdec-41b4-826d-a6a7a895f92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bos_token': '<s>',\n",
              " 'eos_token': '</s>',\n",
              " 'unk_token': '<unk>',\n",
              " 'sep_token': '[SEP]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"日本の首都は\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)\n",
        "pprint.pp(encoded_inputs)"
      ],
      "metadata": {
        "id": "s2dL7jEeVoLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de128826-0867-4682-cb62-2d8035024021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁日本', 'の首都', 'は', '</s>']\n",
            "{'input_ids': tensor([[  491, 11308,    15,     2]]),\n",
            " 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"アメリカの首都はワシントンD.C.です。日本の首都は\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "dHR41K54WBRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d854f33-5e4e-47d3-a18e-3007be3d3657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '日本の', '首都', 'は', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"アメリカの首都はワシントンD.C.です。\", \"日本の首都は\"]\n",
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "0LzqrlatYKC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76136bfb-da14-491b-da7d-d188f26f87cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', '今日', 'は', '良い', '天気', 'です', 'ね', '。', '</s>', '▁', '公園', 'で', '本', 'でも', '読み', 'ましょう', 'か', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"アメリカの首都はワシントンD.C.です。\"\n",
        "text2 = \"日本の首都は\"\n",
        "encoded_inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "uFcsKO3-XLW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cd2d82-7032-4aca-dbee-a689dfd573fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '</s>', '▁日本', 'の首都', 'は', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章を複数入力したい２つのシチュエーションについて\n",
        "\n",
        "1. 一つの出力を得たいが、BERTに文章を区別してもらいたい\n",
        "\n",
        "2. 一つ一つの文章毎に、BERTに特定のタスクを行ってもらいたい"
      ],
      "metadata": {
        "id": "YWzgxUQJL5HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt2-large-japanese\")"
      ],
      "metadata": {
        "id": "B2UAXM_zPMB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一つの出力を得たい"
      ],
      "metadata": {
        "id": "GC-yZfCWO-SS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenizerの引数に２つの文字列を渡すパターン"
      ],
      "metadata": {
        "id": "rXlEpHr8p-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"アメリカの首都はワシントンD.C.です。\"\n",
        "question = \"そして、日本の首都は\""
      ],
      "metadata": {
        "id": "AxEadSv6NIZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(context, question, return_tensors=\"pt\")\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "Dg91v2_bPiCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9ad2b7-56e0-41c4-8fac-9b38bf27e26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '</s>', '▁そして', '、', '日本の', '首都', 'は', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "FcY-fCg5Pnkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "_9ODfDZ2mCAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f75cbe1-61ec-48e8-ae3f-33e2e0915d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['アメリカの首都はワシントンD.C.です。 そして、日本の首都は首都機能が']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### それぞれに出力を得たい（バッチ処理）"
      ],
      "metadata": {
        "id": "eWvOSjtrst5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenizerに配列を渡すパターン"
      ],
      "metadata": {
        "id": "jBFQPBG_qFl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"アメリカの首都はワシントンD.C.です。\"\n",
        "question = \"そして、日本の首都は\"\n",
        "text = [context, question]"
      ],
      "metadata": {
        "id": "7BUdfxTArARI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = \"left\" # デコーダのみのモデルでは、パディングトークンを左に挿入しないと回答が不安定になる\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "for i, input_id_list in enumerate(encoded_inputs[\"input_ids\"]):\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    print(f\"original text: {text[i]}\")\n",
        "    print(f\"model input(decoded): {decoded_tokens}\")\n",
        "    print(f\"attention mask: {encoded_inputs['attention_mask'][i].tolist()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "hDr0yDT9qOqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f666763-f27c-4ba8-ad66-023e66edcf05"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: アメリカの首都はワシントンD.C.です。\n",
            "model input(decoded): ['▁アメリカの', '首都', 'は', 'ワシントン', 'D', '.', 'C', '.', 'です', '。', '</s>']\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------\n",
            "original text: そして、日本の首都は\n",
            "model input(decoded): ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '▁そして', '、', '日本の', '首都', 'は', '</s>']\n",
            "attention mask: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "6TJGAKv6qSft"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "g4lB_xwgqqyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c741a170-bd23-42d9-ea7b-777ca2893619"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['アメリカの首都はワシントンD.C.です。ワシントンD.C.は、アメリカ合衆国の首都', 'そして、日本の首都は首都機能が集中している東京です。 ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (補足)パディングトークンを左詰めで挿入しないと本当に回答は不安定になるのか"
      ],
      "metadata": {
        "id": "IM-YgUr_utmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    \"フランスの首都はパリだし、日本の首都は\",\n",
        "    \"人生良いこともあれば\",\n",
        "    \"お米は炭水化物。お肉は\",\n",
        "    \"日本一高い山の名前は、\",\n",
        "    \"日本の内閣総理大臣の名前は\",\n",
        "    \"日本の冬は乾燥するが、日本の夏は\",\n",
        "]"
      ],
      "metadata": {
        "id": "3z_m7hkKubCl"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 右詰めパディング\n",
        "tokenizer.padding_side = \"right\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)\n",
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "pprint.pp(decoded_output)"
      ],
      "metadata": {
        "id": "xb7u0lfWvjIQ",
        "outputId": "fe22ca1a-bfb4-4c9c-ab12-4f543efb55ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['フランスの首都はパリだし、日本の首都は 首都 と 地方 の',\n",
            " '人生良いこともあれば悪いこともある。 でも、 悪い',\n",
            " 'お米は炭水化物。お肉はタンパク質 と 脂質 が豊富',\n",
            " '日本一高い山の名前は、 富士山 です。 富士山は',\n",
            " '日本の内閣総理大臣の名前は ( ) で表記する。',\n",
            " '日本の冬は乾燥するが、日本の夏は、湿度が高く、湿度が高い']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 左詰めパディング\n",
        "tokenizer.padding_side = \"left\"\n",
        "encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "output = model.generate(**encoded_inputs, max_length=20, pad_token_id=tokenizer.pad_token_id)\n",
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "pprint.pp(decoded_output)"
      ],
      "metadata": {
        "id": "jFnXOtGIwGeZ",
        "outputId": "3d414bee-6298-4b98-809a-1af5f9f0a8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['フランスの首都はパリだし、日本の首都は首都機能は東京に置いている。',\n",
            " '人生良いこともあれば悪いこともある。 でも、 悪い',\n",
            " 'お米は炭水化物。お肉はタンパク質 と 脂質 が豊富',\n",
            " '日本一高い山の名前は、富士山(標高2,776m',\n",
            " '日本の内閣総理大臣の名前は: 安倍晋三 である。 首相',\n",
            " '日本の冬は乾燥するが、日本の夏は湿度が高く、湿度が低いの']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oS8gH12wJbr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}